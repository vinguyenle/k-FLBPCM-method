{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Called user_settings.py \n",
      " ==============================================================================\n",
      " Using CLS: 'RBF_SVM' , FE: 'rui-filtered', PP: 'pass' \n",
      " gamma= 0.000010, C=30 , k = None\n",
      " Generating model for e_method = 'rui-filtered' , c_method ='RBF_SVM' , work_dir = 'default' \n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle \n",
      "\n",
      "[Test 0/4]: Accuracy Score = 0.971, F1 Score = 0.972 \n",
      "\n",
      "[[1497    0    0    1]\n",
      " [   0 1408    9   81]\n",
      " [   0   11 1476   12]\n",
      " [   0   54    3 1441]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1498\n",
      "        can       0.96      0.94      0.95      1498\n",
      "       corn       0.99      0.98      0.99      1499\n",
      "     radish       0.94      0.96      0.95      1498\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle \n",
      "\n",
      "[Test 1/4]: Accuracy Score = 0.970, F1 Score = 0.970 \n",
      "\n",
      "[[1498    0    1    0]\n",
      " [   1 1405   10   82]\n",
      " [   0   13 1483    2]\n",
      " [   1   65    3 1429]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1499\n",
      "        can       0.95      0.94      0.94      1498\n",
      "       corn       0.99      0.99      0.99      1498\n",
      "     radish       0.94      0.95      0.95      1498\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle \n",
      "\n",
      "[Test 2/4]: Accuracy Score = 0.974, F1 Score = 0.974 \n",
      "\n",
      "[[1496    0    2    1]\n",
      " [   0 1428    7   63]\n",
      " [   1   12 1480    5]\n",
      " [   0   65    2 1431]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1499\n",
      "        can       0.95      0.95      0.95      1498\n",
      "       corn       0.99      0.99      0.99      1498\n",
      "     radish       0.95      0.96      0.95      1498\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle \n",
      "\n",
      "[Test 3/4]: Accuracy Score = 0.971, F1 Score = 0.972 \n",
      "\n",
      "[[1497    1    0    0]\n",
      " [   1 1404    7   87]\n",
      " [   2   11 1473   12]\n",
      " [   0   49    1 1448]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1498\n",
      "        can       0.96      0.94      0.95      1499\n",
      "       corn       0.99      0.98      0.99      1498\n",
      "     radish       0.94      0.97      0.95      1498\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle \n",
      "\n",
      "[Test 4/4]: Accuracy Score = 0.975, F1 Score = 0.975 \n",
      "\n",
      "[[1498    0    0    0]\n",
      " [   2 1425    4   67]\n",
      " [   0    6 1480   12]\n",
      " [   0   59    2 1437]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1498\n",
      "        can       0.96      0.95      0.95      1498\n",
      "       corn       1.00      0.99      0.99      1498\n",
      "     radish       0.95      0.96      0.95      1498\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5992\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      " *****************************************************************************************\n",
      "  model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle  \n",
      "  scores: accuracy=0.972, f1=0.972\n",
      " *****************************************************************************************\n",
      " ==============================================================================\n",
      " Using CLS: 'RBF_SVM' , FE: 'rui-filtered', PP: 'cmask' \n",
      " gamma= 0.000010, C=30 , k = None\n",
      " Generating model for e_method = 'rui-filtered' , c_method ='RBF_SVM' , work_dir = 'default' \n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle \n",
      "\n",
      "[Test 0/4]: Accuracy Score = 0.969, F1 Score = 0.969 \n",
      "\n",
      "[[1496    1    1    0]\n",
      " [   0 1416    3   79]\n",
      " [   0    6 1491    2]\n",
      " [   1   86    5 1406]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1498\n",
      "        can       0.94      0.95      0.94      1498\n",
      "       corn       0.99      0.99      0.99      1499\n",
      "     radish       0.95      0.94      0.94      1498\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle \n",
      "\n",
      "[Test 1/4]: Accuracy Score = 0.970, F1 Score = 0.970 \n",
      "\n",
      "[[1496    2    1    0]\n",
      " [   2 1420    2   74]\n",
      " [   0    3 1492    3]\n",
      " [   0   88    7 1403]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1499\n",
      "        can       0.94      0.95      0.94      1498\n",
      "       corn       0.99      1.00      0.99      1498\n",
      "     radish       0.95      0.94      0.94      1498\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle \n",
      "\n",
      "[Test 2/4]: Accuracy Score = 0.973, F1 Score = 0.973 \n",
      "\n",
      "[[1491    4    3    1]\n",
      " [   0 1430    9   59]\n",
      " [   0    4 1492    2]\n",
      " [   0   79    3 1416]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      0.99      1.00      1499\n",
      "        can       0.94      0.95      0.95      1498\n",
      "       corn       0.99      1.00      0.99      1498\n",
      "     radish       0.96      0.95      0.95      1498\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle \n",
      "\n",
      "[Test 3/4]: Accuracy Score = 0.969, F1 Score = 0.969 \n",
      "\n",
      "[[1497    0    1    0]\n",
      " [   1 1418    3   77]\n",
      " [   0    8 1489    1]\n",
      " [   0   88    6 1404]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1498\n",
      "        can       0.94      0.95      0.94      1499\n",
      "       corn       0.99      0.99      0.99      1498\n",
      "     radish       0.95      0.94      0.94      1498\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle \n",
      "\n",
      "[Test 4/4]: Accuracy Score = 0.971, F1 Score = 0.971 \n",
      "\n",
      "[[1495    0    3    0]\n",
      " [   1 1419    6   72]\n",
      " [   0    6 1489    3]\n",
      " [   0   72    9 1417]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1498\n",
      "        can       0.95      0.95      0.95      1498\n",
      "       corn       0.99      0.99      0.99      1498\n",
      "     radish       0.95      0.95      0.95      1498\n",
      "\n",
      "avg / total       0.97      0.97      0.97      5992\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      " *****************************************************************************************\n",
      "  model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle  \n",
      "  scores: accuracy=0.970, f1=0.970\n",
      " *****************************************************************************************\n",
      " ==============================================================================\n",
      " Using CLS: 'RBF_SVM' , FE: 'rui-filtered', PP: 'comb' \n",
      " gamma= 0.000010, C=30 , k = 0.2\n",
      " Generating model for e_method = 'rui-filtered' , c_method ='RBF_SVM' , work_dir = 'default' \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle \n",
      "\n",
      "[Test 0/4]: Accuracy Score = 0.984, F1 Score = 0.984 \n",
      "\n",
      "[[1497    0    1    0]\n",
      " [   0 1453    2   43]\n",
      " [   0    1 1498    0]\n",
      " [   0   45    1 1452]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1498\n",
      "        can       0.97      0.97      0.97      1498\n",
      "       corn       1.00      1.00      1.00      1499\n",
      "     radish       0.97      0.97      0.97      1498\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle \n",
      "\n",
      "[Test 1/4]: Accuracy Score = 0.985, F1 Score = 0.985 \n",
      "\n",
      "[[1499    0    0    0]\n",
      " [   0 1462    1   35]\n",
      " [   0    3 1495    0]\n",
      " [   0   51    1 1446]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1499\n",
      "        can       0.96      0.98      0.97      1498\n",
      "       corn       1.00      1.00      1.00      1498\n",
      "     radish       0.98      0.97      0.97      1498\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle \n",
      "\n",
      "[Test 2/4]: Accuracy Score = 0.988, F1 Score = 0.988 \n",
      "\n",
      "[[1496    1    1    1]\n",
      " [   0 1468    5   25]\n",
      " [   0    1 1497    0]\n",
      " [   0   37    1 1460]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1499\n",
      "        can       0.97      0.98      0.98      1498\n",
      "       corn       1.00      1.00      1.00      1498\n",
      "     radish       0.98      0.97      0.98      1498\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle \n",
      "\n",
      "[Test 3/4]: Accuracy Score = 0.986, F1 Score = 0.986 \n",
      "\n",
      "[[1497    1    0    0]\n",
      " [   0 1457    4   38]\n",
      " [   0    3 1495    0]\n",
      " [   0   37    0 1461]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1498\n",
      "        can       0.97      0.97      0.97      1499\n",
      "       corn       1.00      1.00      1.00      1498\n",
      "     radish       0.97      0.98      0.97      1498\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5993\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "Model: model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle \n",
      "\n",
      "[Test 4/4]: Accuracy Score = 0.988, F1 Score = 0.988 \n",
      "\n",
      "[[1497    0    1    0]\n",
      " [   1 1469    2   26]\n",
      " [   0    2 1496    0]\n",
      " [   0   40    0 1458]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " background       1.00      1.00      1.00      1498\n",
      "        can       0.97      0.98      0.98      1498\n",
      "       corn       1.00      1.00      1.00      1498\n",
      "     radish       0.98      0.97      0.98      1498\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5992\n",
      "\n",
      " ------------------------------------------------------------------------------\n",
      "\n",
      " *****************************************************************************************\n",
      "  model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle  \n",
      "  scores: accuracy=0.986, f1=0.986\n",
      " *****************************************************************************************\n",
      "\n",
      " ______________________________________________________________________________________\n",
      "\n",
      "          Contents of Global results: \n",
      "0.9863 model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9861 model_CLS-kernel-rbf-C-100-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9861 model_CLS-kernel-rbf-C-100-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9861 model_CLS-kernel-rbf-C-60-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9860 model_CLS-kernel-rbf-C-100-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9858 model_CLS-kernel-rbf-C-60-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9857 model_CLS-kernel-rbf-C-60-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9856 model_CLS-kernel-rbf-C-1000-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9856 model_CLS-kernel-rbf-C-30-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9851 model_CLS-kernel-rbf-C-1000-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9851 model_CLS-kernel-rbf-C-30-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9850 model_CLS-kernel-rbf-C-1000-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9850 model_CLS-kernel-rbf-C-10-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9849 model_CLS-kernel-rbf-C-1000-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9847 model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9846 model_CLS-kernel-rbf-C-10-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9845 model_CLS-kernel-rbf-C-100-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9845 model_CLS-kernel-rbf-C-60-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9844 model_CLS-kernel-rbf-C-100-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9843 model_CLS-kernel-rbf-C-1000-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9836 model_CLS-kernel-rbf-C-10-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9834 model_CLS-kernel-rbf-C-60-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9825 model_CLS-kernel-rbf-C-60-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9824 model_CLS-kernel-rbf-C-30-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9823 model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9822 model_CLS-kernel-rbf-C-10-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9821 model_CLS-kernel-rbf-C-100-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9818 model_CLS-kernel-rbf-C-1000-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9817 model_CLS-kernel-rbf-C-100-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9817 model_CLS-kernel-rbf-C-60-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9816 model_CLS-kernel-rbf-C-1000-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9811 model_CLS-kernel-rbf-C-1000-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9809 model_CLS-kernel-rbf-C-10-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9803 model_CLS-kernel-rbf-C-10-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9802 model_CLS-kernel-rbf-C-30-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9793 model_CLS-kernel-rbf-C-10-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9792 model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9791 model_CLS-kernel-rbf-C-1000-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9791 model_CLS-kernel-rbf-C-100-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9791 model_CLS-kernel-rbf-C-60-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9790 model_CLS-kernel-rbf-C-100-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9780 model_CLS-kernel-rbf-C-60-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9779 model_CLS-kernel-rbf-C-1000-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9776 model_CLS-kernel-rbf-C-1-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9773 model_CLS-kernel-rbf-C-10-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9764 model_CLS-kernel-rbf-C-1-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9758 model_CLS-kernel-rbf-C-30-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9736 model_CLS-kernel-rbf-C-1-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9733 model_CLS-kernel-rbf-C-100-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9725 model_CLS-kernel-rbf-C-10-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9723 model_CLS-kernel-rbf-C-1000-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9723 model_CLS-kernel-rbf-C-100-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9723 model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9722 model_CLS-kernel-rbf-C-60-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9720 model_CLS-kernel-rbf-C-100-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9719 model_CLS-kernel-rbf-C-10-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9718 model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9718 model_CLS-kernel-rbf-C-10-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9717 model_CLS-kernel-rbf-C-1000-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9717 model_CLS-kernel-rbf-C-100-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9717 model_CLS-kernel-rbf-C-60-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9714 model_CLS-kernel-rbf-C-60-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9712 model_CLS-kernel-rbf-C-60-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9710 model_CLS-kernel-rbf-C-1-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9708 model_CLS-kernel-rbf-C-1000-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9704 model_CLS-kernel-rbf-C-30-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9697 model_CLS-kernel-rbf-C-30-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9688 model_CLS-kernel-rbf-C-1-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9677 model_CLS-kernel-rbf-C-1-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.8.pickle\n",
      "\n",
      "0.9667 model_CLS-kernel-rbf-C-10-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9664 model_CLS-kernel-rbf-C-1000-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9656 model_CLS-kernel-rbf-C-1-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-1.pickle\n",
      "\n",
      "0.9651 model_CLS-kernel-rbf-C-10-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9651 model_CLS-kernel-rbf-C-1-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.5.pickle\n",
      "\n",
      "0.9634 model_CLS-kernel-rbf-C-1-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9601 model_CLS-kernel-rbf-C-100-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9579 model_CLS-kernel-rbf-C-60-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9576 model_CLS-kernel-rbf-C-1-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-pass.pickle\n",
      "\n",
      "0.9550 model_CLS-kernel-rbf-C-1-gamma-1e-05_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9549 model_CLS-kernel-rbf-C-1-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.2.pickle\n",
      "\n",
      "0.9545 model_CLS-kernel-rbf-C-30-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9496 model_CLS-kernel-rbf-C-10-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "0.9470 model_CLS-kernel-rbf-C-1-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-comb-thick-2-k-0.1.pickle\n",
      "\n",
      "0.9314 model_CLS-kernel-rbf-C-1-gamma-1e-06_FE-lbp-rui-filtered-r1-n8-r2-n16-r3-n24PP-cmask-thick-2.pickle\n",
      "\n",
      "\n",
      "ALL DONE\n"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "#                                                                                    #\n",
    "# An effective and novel k-LBPCM method for detecting morphologically similar plants #\n",
    "# based on the combination of contour masks and Local Binary Pattern operators       #\n",
    "#                                                                                    #\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "# import the necessary packages\n",
    "import numpy as np \n",
    "import helper_functions\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20,20)\n",
    "global do_debug\n",
    "\n",
    "# --------- Set to true if want to see the first 20 LBP images with features matrix\n",
    "do_debug = False\n",
    "#do_debug = True\n",
    "\n",
    "class features_class:\n",
    "    def __init__(self,X,y,img_names):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.img_names=img_names\n",
    "        self.base_path=\"\"\n",
    "        self.set_start_i=[] # indices in the features list for each dataset\n",
    "        self.set_stop_i=[]\n",
    "        self.md5=\"\" \n",
    "                \n",
    "class image_preprocess:\n",
    "    #Constructor\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        \n",
    "        method: \"pass \" -> no transformation is applied\n",
    "                \"contours\" -> mask input image with contours\n",
    "                \"cmask\" -> generate mask then apply to features matrix\n",
    "                \n",
    "        thickness : Only used when method is \"contours\" or \"cmask\"\n",
    "    \n",
    "    Functions:\n",
    "        process (image_path, path_to_save)\n",
    "            inputs: image_path -> path for image to process. NOTE: it needs to have \"work\" in the path\n",
    "            output: image_out -> output image. This will also be saved to disk if it does not exist\n",
    "    \"\"\"\n",
    "    def __init__(self, method=\"pass\", params=[]):\n",
    "        self.method   = method\n",
    "        self.params   = params\n",
    "        self.post_process_mask = None\n",
    "        if len(params)>0:\n",
    "            self.thickness = params['thick'] # first param is thickness\n",
    "        if len(params)>1:\n",
    "            self.k = params['k'] # second param is gain\n",
    "               \n",
    "    def process (self, image_path):      \n",
    "        binary_threshold = 40\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "        import os\n",
    "\n",
    "        if self.method == \"pass\":\n",
    "            # [ Read image from path and return it unchanged]\n",
    "            image =  cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            return image\n",
    "        elif self.method == \"contours\":\n",
    "            # check if file is already generated\n",
    "            if not \"work\" in image_path:\n",
    "                print \"ERROR: make sure the images have 'work' in the path directory\"\n",
    "                return None\n",
    "            out_filename = image_path.replace(\"work\",\"work_contours_T-\" + str(self.thickness))\n",
    "            if os.path.isfile(out_filename):\n",
    "                # If file exists, simply load it\n",
    "                my_out_img=  cv2.imread(out_filename, cv2.IMREAD_GRAYSCALE)\n",
    "                return my_out_img\n",
    "            else:\n",
    "                # File does not exist, generate it\n",
    "                orig_img   = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                elem_open  = cv2.getStructuringElement(cv2.MORPH_RECT,(5,5)) # create a 5x5 processing filter\n",
    "                                                                             # for opening \n",
    "                elem_close = cv2.getStructuringElement(cv2.MORPH_RECT,(3,3))\n",
    "                img_open   = cv2.morphologyEx(orig_img,cv2.MORPH_OPEN ,elem_open ) # Image with open morphological op\n",
    "\n",
    "                img_filt        = cv2.morphologyEx(img_open,cv2.MORPH_CLOSE,elem_close) # Closing morphological op\n",
    "                # Threshold to binary\n",
    "                ret, im_bin     = cv2.threshold(img_filt,binary_threshold,1,cv2.THRESH_BINARY)\n",
    "                # Create contours using the binary image\n",
    "                if \"4.0.0\" in cv2.__version__:\n",
    "                    my_contours, hierarchy = cv2.findContours(im_bin.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                else:\n",
    "                    im2, my_contours, hierarchy = cv2.findContours(im_bin.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                # Write a simple contours mask\n",
    "                #print 'self.thickness = ', self.thickness\n",
    "                mask_ext = cv2.drawContours(im_bin.copy(),my_contours,-1,255,self.thickness)\n",
    "                mask = cv2.drawContours(mask_ext.copy(),my_contours,-1,100,2)\n",
    "                my_masked_orig = np.where((mask_ext==255),orig_img,0)\n",
    "\n",
    "                # Check if the out path is differnt from source ... Otherwise something has gone wrong\n",
    "                if out_filename != image_path:\n",
    "                    print 'Creating: ', out_filename\n",
    "                    base_dir = os.path.split(out_filename)[0]\n",
    "                    if not os.path.isdir(base_dir):\n",
    "                        os.makedirs(base_dir) # create dir if it doesn't exist\n",
    "                    cv2.imwrite(out_filename, my_masked_orig)\n",
    "                # [ Return the image with masked contours]\n",
    "                return my_masked_orig\n",
    "            \n",
    "        elif self.method == \"cmask\" or self.method== \"comb\":\n",
    "            # -----------\n",
    "            #  This method reads an image, generates a contours mask and\n",
    "            #      - returns the original image (unchanged)\n",
    "            #      - stores the contours mask as a local variable\n",
    "            # -----------\n",
    "            # check if file is already generated\n",
    "            if not \"work\" in image_path:\n",
    "                print \"ERROR: make sure the images have 'work' in the path directory\"\n",
    "                return None\n",
    "            out_filename = image_path.replace(\"work\",\"work_cmask_T-\" + str(self.thickness))\n",
    "            if os.path.isfile(out_filename):\n",
    "                # If file exists, simply load it\n",
    "                self.post_process_mask =  cv2.imread(out_filename, cv2.IMREAD_GRAYSCALE)\n",
    "                orig_img   = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                return orig_img\n",
    "            else:\n",
    "                # File does not exist, generate it\n",
    "                orig_img   = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if orig_img is None:\n",
    "                    raise Exception(\" Could not load image '%s'\" %(image_path))\n",
    "                elem_open  = cv2.getStructuringElement(cv2.MORPH_RECT,(5,5)) # create a 5x5 processing filter\n",
    "                                                                             # for opening \n",
    "                elem_close = cv2.getStructuringElement(cv2.MORPH_RECT,(3,3))\n",
    "                img_open   = cv2.morphologyEx(orig_img.copy(),cv2.MORPH_OPEN ,elem_open ) # Image with open morphological op\n",
    "\n",
    "                img_filt        = cv2.morphologyEx(img_open,cv2.MORPH_CLOSE,elem_close) # Closing morphological op\n",
    "                # Threshold to binary\n",
    "                ret, im_bin     = cv2.threshold(img_filt,binary_threshold,1,cv2.THRESH_BINARY)\n",
    "                # Create contours using the binary image\n",
    "                if \"4.0.0\" in cv2.__version__:\n",
    "                    my_contours, hierarchy = cv2.findContours(im_bin.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                else:\n",
    "                    im2, my_contours, hierarchy = cv2.findContours(im_bin.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                # Write a simple contours mask\n",
    "                #print 'self.thickness = ', self.thickness\n",
    "                mask = cv2.drawContours(im_bin.copy(),my_contours,-1,255,self.thickness)\n",
    "\n",
    "                # Note:    for the combined mask (or post_process_mask)\n",
    "                #   mask value of 255 -> contour of given thickness\n",
    "                #   mask value of   1 -> segmented pixel found to be \"green\" as per original image\n",
    "                #   mask value of   0 -> segmented pixel not green\n",
    "                \n",
    "                # Mask original image:  - Value of 0 where there is no contour\n",
    "                #                       - Pixel value from orig_img if it's on the masked contour\n",
    "                \n",
    "                # Check if the out path is differnt from source ... Otherwise something has gone wrong\n",
    "                if out_filename != image_path:\n",
    "                    print 'Creating: ', out_filename\n",
    "                    base_dir = os.path.split(out_filename)[0]\n",
    "                    if not os.path.isdir(base_dir):\n",
    "                        os.makedirs(base_dir) # create dir if it doesn't exist\n",
    "                    cv2.imwrite(out_filename, mask)\n",
    "                # [ Return the original mage and set the post process mask]\n",
    "                self.post_process_mask = mask\n",
    "                return orig_img\n",
    "\n",
    "class plant_feature_extraction:\n",
    "    def __init__(self,method,params):\n",
    "        # Given method can also a features filter argument. We don't want to put the filter\n",
    "        # arguments in our self.params vector as we want the same name file for the extracted\n",
    "        # features. This way, we'll have the same features files for all methods and\n",
    "        # save computational time.\n",
    "        \n",
    "        self.method_and_filter = method\n",
    "        self.method = method.split('-filtered')[0] # first split is actual extractor method\n",
    "                \n",
    "        if '-filtered' in self.method_and_filter:\n",
    "            self.features_filter = 'filtered-' + self.method_and_filter.split('-filtered')[1]# second split is fitlter arguments\n",
    "        else:\n",
    "            self.features_filter = 'none'\n",
    "        self.params = params\n",
    "        # ----------------------------\n",
    "        # Internal variables which can change after class instantiation\n",
    "        self.file_base_name = None  # lbp_rui_r1_n8_r2_n16_r3_n24\n",
    "        self.features_file_name = None  # this will be updated when calling initialise()\n",
    "                                        # according to the latest operators      \n",
    "        self.n_bins         = None\n",
    "        self.e_params       = None\n",
    "        self.is_initialised = None\n",
    "        \n",
    "        self.work_dir       = None\n",
    "        self.dataset_lists  = []        # This will hold the dataset filenames (i.e. dataset_list_1..5) \n",
    "        \n",
    "    def initialise(self, preprocessor ,work_dir):\n",
    "        import helper_functions as hlp\n",
    "        from natsort import natsorted\n",
    "        \n",
    "        # Define our dataset training/testing names\n",
    "        self.file_base_name      = 'lbp-' + self.method\n",
    "        # Calculate internal variables according to given methods\n",
    "        pp_base_name='PP-' + preprocessor.method\n",
    "        \n",
    "        for my_operator in  self.params:\n",
    "            self.file_base_name += '-r%d-n%d' %( my_operator[0],my_operator[1])        \n",
    "\n",
    "        for key in preprocessor.params:\n",
    "            pp_base_name += '-' + key + '-' + str(preprocessor.params[key]) # String containing all our params and values\n",
    "                                                                            # so that we don't need to regenerate\n",
    "        self.features_file_name  = 'features_' +'FE-'+ self.file_base_name + '_' + pp_base_name + '.pickle'\n",
    "        self.file_base_name +=  pp_base_name         \n",
    "        self.work_dir        = work_dir\n",
    "        self.out_dir         = work_dir.replace('work','out')    \n",
    "        self.nprocessed = 0;\n",
    "        \n",
    "        # Update dataset lists\n",
    "        dataset_lists = hlp.list_files(self.work_dir,name_prefix=\"dataset_list\" ,file_ext='txt')\n",
    "        self.dataset_lists = natsorted(dataset_lists)# just keep files in alphanumnerical order\n",
    "        if do_debug:\n",
    "            print ' DEBUG: Dataset lists are:', self.dataset_lists\n",
    "        if len(self.dataset_lists) == 0:\n",
    "            raise Exception(\" Dataset is empty! No valid files found in '%s'\" %(self.work_dir))\n",
    "        self.is_initialised = 1\n",
    "        \n",
    "    def get_features_histogram (self, image, pp):\n",
    "        import numpy as np\n",
    "        from skimage import feature\n",
    "        \n",
    "        bins_combined = []\n",
    "        method = self.method.replace('-filtered','')\n",
    "        self.nprocessed+=1;\n",
    "        if method == 'rui':\n",
    "            for r_n_param in self.params:\n",
    "                r = r_n_param[0]\n",
    "                n = r_n_param[1]\n",
    "                fill_value = 255 # |NOTE THIS APPEARS in the LBP image when masking but we'll ignore as n+2 <255\n",
    "                my_lbp_matrix  = feature.local_binary_pattern(image, n, r, method=\"uniform\")\n",
    "                if pp.post_process_mask is not None:\n",
    "                    masked_lbp_matrix  = np.where((pp.post_process_mask==255),my_lbp_matrix,fill_value)\n",
    "                else:\n",
    "                    masked_lbp_matrix = my_lbp_matrix\n",
    "                # Just a simple debug window into what's going on. MAKE sure you disable for the long run\n",
    "                if do_debug and self.nprocessed < 10:\n",
    "                    plt.subplot(1,2,1)\n",
    "                    plt.imshow(image)\n",
    "                    plt.title('Orignal')\n",
    "                    plt.subplot(1,2,2)\n",
    "                    plt.imshow(masked_lbp_matrix)\n",
    "                    if pp.post_process_mask is None:\n",
    "                        plt.title('LBP matrix')\n",
    "                    else:\n",
    "                        plt.title('Masked LBP matrix')\n",
    "                    plt.show()\n",
    "                    self.debug_last_mask= pp.post_process_mask\n",
    "                    self.debug_last_lbp_matrix = my_lbp_matrix\n",
    "                    self.debug_last_masked_lbp_matrix = masked_lbp_matrix\n",
    "                    self.debug_last_image = image\n",
    "                \n",
    "                my_bins, my_edges = np.histogram(masked_lbp_matrix.ravel(),bins=np.arange(0, n+3), range=(0,n+2))\n",
    "                if pp.method == \"comb\":\n",
    "                    bins_from_full_img, my_edges = np.histogram(my_lbp_matrix.ravel(),bins=np.arange(0, n+3), range=(0,n+2))\n",
    "                    my_bins = my_bins + (bins_from_full_img * pp.k).astype('int64');\n",
    "                bins_combined = np.concatenate( (bins_combined,my_bins) ,axis=0)\n",
    "                #print 'DEBUG: %s , n= %d, r = %d ; n_bins = %d' %(str(r_n_param),n,r,len(bins_combined) )                \n",
    "        self.n_bins = len(bins_combined)\n",
    "        return bins_combined\n",
    "    \n",
    "    def apply_features_filter(self, given_features):\n",
    "        if self.features_filter is not None:    \n",
    "            X = np.delete(given_features.X,[8,26,52],1) #remove 9th, 27th, 53th bins in LBP operators\n",
    "            given_features.X = X\n",
    "            return given_features\n",
    "\n",
    "    def check_features_file(self,pp, out_filename):\n",
    "        \"\"\"\n",
    "            This function generates the comb method test/train features file from \n",
    "            existing pass and cmask pickle files (so that we don't have to re-compute for a change of\n",
    "            k parameter). The formula used is: \n",
    "            comb_features = cmask_features + k* pass_features\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import pickle\n",
    "        # check if file already exists\n",
    "        if os.path.isfile(out_filename):\n",
    "            return\n",
    "        if pp.method == \"comb\":\n",
    "            # this method takes 2 files of generated features and combines them\n",
    "            # so check if these 2 files exist\n",
    "            base_name = out_filename.split('comb-')[0]\n",
    "            pass_features_filename = base_name + \"pass.pickle\"\n",
    "            cmask_features_filename = base_name + \"cmask-thick-\"+str(pp.thickness) + \".pickle\"\n",
    "            if os.path.isfile(pass_features_filename) and os.path.isfile(cmask_features_filename):\n",
    "                print ' Generating %s file from existing cmask and pass feature files' %(out_filename)\n",
    "                with open (pass_features_filename,'rb') as f:\n",
    "                    my_features_pass = pickle.load(f)\n",
    "                        \n",
    "                    print \"From pass file: Loaded %d images x %d features \" %(len(my_features_pass.X), len(my_features_pass.X[1]))\n",
    "                with open (cmask_features_filename,'rb') as f:\n",
    "                    my_features_cmask = pickle.load(f)\n",
    "                    print \"From cmask file: Loaded %d images x %d features \" %(len(my_features_cmask.X), len(my_features_cmask.X[1]))\n",
    "\n",
    "                X = (np.asarray(my_features_cmask.X).copy()).astype('float')\n",
    "                X = X + np.asarray(my_features_pass.X).copy() * pp.k\n",
    "                \n",
    "                # Now assign the other variables which need to be written in the dump file\n",
    "                my_labels      = my_features_pass.y\n",
    "                my_img_names   = my_features_pass.img_names\n",
    "                base_path      = my_features_pass.base_path\n",
    "                my_set_start_i = my_features_pass.set_start_i\n",
    "                my_set_stop_i  = my_features_pass.set_stop_i\n",
    "                md5_sum        = my_features_pass.md5\n",
    "                print 'DEBUG', my_features_pass.set_stop_i\n",
    "                print 'DEBUG start', my_features_pass.set_start_i\n",
    "                with open(out_filename,'wb') as f:\n",
    "                    my_features = features_class(X=np.asarray(X).astype('float'),\\\n",
    "                                                 y=my_labels,\\\n",
    "                                                 img_names=my_img_names)\n",
    "                    my_features.base_path = base_path\n",
    "                    my_features.md5       = md5_sum\n",
    "                    my_features.set_start_i = my_set_start_i;\n",
    "                    my_features.set_stop_i  = my_set_stop_i;                    \n",
    "                    pickle.dump(my_features,f, pickle.HIGHEST_PROTOCOL)\n",
    "                    print 'Written %s file' %(out_filename)\n",
    "            else:\n",
    "                if not os.path.isfile(pass_features_filename):\n",
    "                    print \"File '%s' does not exist ... will generate from scratch\" %(pass_features_filename)\n",
    "                if not os.path.isfile(cmask_features_filename):\n",
    "                    print \"File '%s' does not exist ... will generate from scratch\" %(cmask_features_filename)\n",
    "                print ' Generating %s file from scratch' %(out_filename)\n",
    "            \n",
    "    def extract_features(self, preprocess_class, work_dir = \"\", overwrite =0):\n",
    "        import os\n",
    "        import cv2\n",
    "        import helper_functions as hlp\n",
    "        import pickle\n",
    "        import hashlib\n",
    "        \n",
    "        if self.is_initialised is None:\n",
    "            self.initialise(preprocess_class, work_dir)\n",
    "            \n",
    "        feature_extraction_file_name = self.features_file_name\n",
    "        base_directory = work_dir\n",
    "        if not os.path.isdir(base_directory):\n",
    "            print \" ERROR !!! Given input directory '%s' does not exist !!!! \" % base_directory\n",
    "            return\n",
    "        \n",
    "        if not os.path.isdir(self.out_dir):\n",
    "            os.mkdir(self.out_dir)\n",
    "        out_filename = self.out_dir + '/' + feature_extraction_file_name\n",
    "        \n",
    "        self.check_features_file(preprocess_class, out_filename) # check if we can generate from other features files\n",
    "        if not len(self.dataset_lists):\n",
    "            raise Exception('Dataset list is empty')\n",
    "\n",
    "        md5_sum          = hashlib.md5(self.dataset_lists[0]).hexdigest() # a simple check for the first list \n",
    "\n",
    "        # Check if file already exists\n",
    "        if not (os.path.isfile(out_filename) and (overwrite ==0)):\n",
    "            my_bins_combined = list()\n",
    "            my_labels        = list()\n",
    "            my_img_names     = list()\n",
    "            my_set_start_i =[] # indices for each set so that we don't have to regenerate or lookup\n",
    "            my_set_stop_i   =[]\n",
    "            for i in range(len(self.dataset_lists)):\n",
    "                img_list = base_directory + '/' + self.dataset_lists[i]\n",
    "                with open(img_list,'r') as dataset_file:\n",
    "                    dataset_details = dataset_file.readlines()\n",
    "                    print ' ----------------- Processing %s -------------' %(img_list)\n",
    "                    my_set_start_i.append(len(my_labels))\n",
    "                    print '   ... [%d] found %d files, start_i = %d ' %(i,len(dataset_details),my_set_start_i[-1])\n",
    "\n",
    "                    for data_line in dataset_details:\n",
    "                        # Expecting format : <label> <filename>\n",
    "                        # E.g.: corn cornS1-r8-266_b1.bmp\n",
    "                        line_splits = data_line.split(' ')\n",
    "                        if len(line_splits) < 2 :\n",
    "                            print (\" ERROR!  Data format not recognised in %s \" %(img_list))\n",
    "                            print (\" Line '%s' doesn't fit format '<label> <img_name>' \")\n",
    "                            raise (\" Dataset file format not as expected \")\n",
    "                        else:\n",
    "                            my_label = line_splits[0]\n",
    "                            my_file  = line_splits[1].strip() # strip the end of line\n",
    "                            img_path = base_directory + '/' + my_label + '/'+ my_file\n",
    "                            image = preprocess_class.process(img_path)\n",
    "                            if image is None:\n",
    "                                print ' Error: preprocess_class.process returned no image for %s !' %(img_path)\n",
    "                            bins_combined = self.get_features_histogram(image, preprocess_class)\n",
    "                            my_bins_combined.append(bins_combined)\n",
    "                            my_labels.append(my_label)\n",
    "                            my_img_names.append(my_file)\n",
    "                    my_set_stop_i.append(len(my_labels))\n",
    "                            \n",
    "            with open(out_filename, 'wb') as f: \n",
    "                print \" Processing done ... writing pickle file '%s' md5='%s' \" %(out_filename,md5_sum)                    \n",
    "                my_features = features_class(X=np.asarray(my_bins_combined).astype('float'),\\\n",
    "                                             y=my_labels,\\\n",
    "                                             img_names=my_img_names)\n",
    "                my_features.base_path = base_directory\n",
    "                my_features.md5       = md5_sum\n",
    "                my_features.set_start_i = my_set_start_i;\n",
    "                my_features.set_stop_i   = my_set_stop_i;\n",
    "                if len(my_set_start_i) != len(my_set_stop_i):\n",
    "                    raise Exception('Something went wrong, start/end indices for lists have diff length')\n",
    "                pickle.dump(my_features,f, pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "    def read_features(self):\n",
    "        import pickle\n",
    "        my_features = None\n",
    "        features_file_name = self.out_dir + '/' + self.features_file_name\n",
    "        with open (features_file_name,'rb') as f:\n",
    "            my_features = pickle.load(f)\n",
    "                                 \n",
    "        return self.apply_features_filter(my_features)\n",
    "                                 \n",
    "    def read_train_features(self,index_id):\n",
    "        '''\n",
    "            index_id: N -> using set N for testing and all other sets for training \n",
    "        '''\n",
    "        features = self.read_features()\n",
    "        \n",
    "        if len(features.set_start_i) ==0:\n",
    "            raise Exception('Features sets indices are 0 ... is this the original set?')\n",
    "        if index_id+1 > len(features.set_stop_i):\n",
    "            raise Exception('Feature file index %d is out of features range (max %d)' %( index_id,\n",
    "                           len(features.set_stop_i)-1) )#\n",
    "        start_i = features.set_start_i[index_id]\n",
    "        stop_i   = features.set_stop_i[index_id]\n",
    "        \n",
    "        if len(features.X) < stop_i:\n",
    "            raise Exception(' Something is not right with this features dataset. stop_i=%d, Len=%d'\\\n",
    "                           %(len(features.X), stop_i+1))\n",
    "        features.X = np.delete(features.X,np.r_[start_i:stop_i+1],0)\n",
    "        features.y = np.delete(features.y,np.r_[start_i:stop_i+1],0)\n",
    "        features.img_names = np.delete(features.img_names,np.r_[start_i:stop_i],0)\n",
    "        # delete indices just in case we're thinking of reusing this array\n",
    "        features.set_start_i=[]\n",
    "        features.set_stop_i = []\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    def read_test_features(self,index_id):\n",
    "        '''\n",
    "            index_id: N -> using set N for testing. Return only selected test features\n",
    "        '''        \n",
    "        features = self.read_features()        \n",
    "        if len(features.set_start_i) ==0:\n",
    "            raise Exception('Features sets indices are 0 ... is this the original set?')\n",
    "        start_i = features.set_start_i[index_id]\n",
    "        stop_i   = features.set_stop_i[index_id]\n",
    "        \n",
    "        features.X = np.copy(features.X[start_i:stop_i+1])\n",
    "        features.y = np.copy(features.y[start_i:stop_i+1])\n",
    "        features.img_names = np.copy(features.img_names[start_i:stop_i])\n",
    "        # delete indices just in case we're thinking of reusing this array\n",
    "        features.set_start_i=[]\n",
    "        features.set_stop_i = []\n",
    "        return features    \n",
    "                    \n",
    "class plant_classifier:\n",
    "    def __init__(self,method,params):\n",
    "        from sklearn.svm import SVC\n",
    "        \n",
    "        self.method = method\n",
    "        self.params = params\n",
    "        \n",
    "        self.out_dir        = None\n",
    "        self.is_initialised = None\n",
    "        \n",
    "        if self.method == 'linear_SVM':\n",
    "                self.my_classifier = SVC( kernel = self.params['kernel'],\n",
    "                                          C      = self.params['C'],\n",
    "                                          gamma  = self.params['gamma']\n",
    "                                        )\n",
    "        \n",
    "        if self.method == 'poly_SVM':\n",
    "                self.my_classifier = SVC( kernel= self.params['kernel'],\n",
    "                                          C     = self.params['C'],\n",
    "                                          gamma = self.params['gamma'],\n",
    "                                          degree= self.params['degree']\n",
    "                                        )\n",
    "            \n",
    "        if self.method == 'RBF_SVM':\n",
    "                self.my_classifier = SVC(kernel = self.params['kernel'],\n",
    "                                         C      = self.params['C'] ,\n",
    "                                         gamma  = self.params['gamma'])\n",
    "                \n",
    "    def initialise(self, feature_extractor):   \n",
    "        # Define our dataset training/testing names\n",
    "        model_base_name ='CLS'\n",
    "        for key in self.params:\n",
    "            model_base_name += '-' + key + '-' + str(self.params[key]) # String containing all our params and values\n",
    "                                                                 # so that we don't need to regenerate\n",
    "        self.file_base_name = model_base_name\n",
    "        # Append feature extractor name ... generate name with filter if any\n",
    "        feature_extractor_name = feature_extractor.file_base_name.replace(feature_extractor.method,\n",
    "                                                                          feature_extractor.method_and_filter)\n",
    "        self.file_base_name += '_FE-' + feature_extractor_name\n",
    "        self.out_dir         =  feature_extractor.work_dir.replace('work','out')\n",
    "\n",
    "        self.model_file_name = 'model_' + self.file_base_name + '.pickle'\n",
    "\n",
    "        self.global_results_file = self.out_dir + '/' + 'global_results.txt'\n",
    "        \n",
    "        self.is_initialised = 1\n",
    "        \n",
    "    def generate_models(self, feature_extractor):\n",
    "        import os\n",
    "        import pickle\n",
    "        dataset_lists = feature_extractor.dataset_lists\n",
    "        if not len(dataset_lists):\n",
    "            raise Exception(' Called generate models with empty dataset_lists')\n",
    "            \n",
    "        for test_i in range(len(dataset_lists)):\n",
    "            model_full_file_name = self.out_dir + '/' + str(test_i) + '_'+ self.model_file_name\n",
    "            # Train model: Check if already exists ... if not, then generate model\n",
    "            if (not os.path.isfile(model_full_file_name) ):\n",
    "                list_path =  dataset_lists[test_i]\n",
    "                print\" Creating a prediction model file: '%s' \" % model_full_file_name\n",
    "\n",
    "                my_features = feature_extractor.read_train_features(test_i)   \n",
    "                model = self.my_classifier.fit(my_features.X, my_features.y)\n",
    "                filename = model_full_file_name\n",
    "                print 'Writting file ...'\n",
    "                with open(model_full_file_name, 'wb') as f:\n",
    "                    pickle.dump(model,f, pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "    def train(self, feature_extractor, overwrite=0 ):\n",
    "        import os\n",
    "        \n",
    "        if feature_extractor.is_initialised is None:\n",
    "            print ' Please initialise the feature extractor first before calling the classifier'\n",
    "            return\n",
    "        \n",
    "        self.initialise(feature_extractor)\n",
    "        if not os.path.isdir(self.out_dir):\n",
    "            os.mkdir(self.out_dir)\n",
    "            print \" Created directory '%s' \" % self.out_dir       \n",
    "        self.generate_models(feature_extractor)\n",
    "          \n",
    "    \n",
    "    def predict(self, feature_extractor ):\n",
    "        import os\n",
    "        import numpy as np\n",
    "        from sklearn.externals import joblib\n",
    "        import pickle \n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import f1_score \n",
    "       \n",
    "        from natsort import natsorted\n",
    "               \n",
    "        if feature_extractor.is_initialised is None:\n",
    "            raise Exception(' Please initialise the feature extractor first before calling the classifier')\n",
    "        \n",
    "        self.initialise(feature_extractor)\n",
    "        dataset_lists = feature_extractor.dataset_lists\n",
    "        if not len(dataset_lists):\n",
    "            raise Exception(' Called generate models with empty dataset_lists')      \n",
    "        \n",
    "        accuracy_test_scores=[]\n",
    "        f1_test_scores=[]\n",
    "        for test_i in range(len(dataset_lists)):            \n",
    "            model_full_file_name = self.out_dir + '/' + str(test_i) + '_'+ self.model_file_name\n",
    "            # Check if the model file name already exists\n",
    "            if (not os.path.isfile(model_full_file_name) )  :\n",
    "                raise Exception(\"Model'%s' does not exist!. Please train model first! \" % (model_full_file_name))\n",
    "\n",
    "            my_features = feature_extractor.read_test_features(test_i)\n",
    "            X= my_features.X\n",
    "            if do_debug:\n",
    "                print ' DEBUG: [%d] Len of features = %d' %(test_i, len(X[0]))\n",
    "                print 'First img (%s), features = %s' %(my_features.img_names[0], str(X[0]))\n",
    "                print 'Last img  (%s), features = %s' %(my_features.img_names[-1],str(X[-1]))\n",
    "                \n",
    "\n",
    "            # Load prediction model\n",
    "            with open (model_full_file_name,'rb') as f:\n",
    "                my_model = pickle.load(f)            \n",
    "                #result = loaded_model.score(X_test, Y_test)\n",
    "                predictions = my_model.predict(X)\n",
    "\n",
    "            current_accuracy_score = accuracy_score(my_features.y, predictions)\n",
    "            current_f1_score = f1_score(my_features.y, predictions,average='weighted')\n",
    "            current_model_name = self.model_file_name\n",
    "            accuracy_test_scores.extend([current_accuracy_score])\n",
    "            f1_test_scores.extend([current_f1_score])\n",
    "            print '\\n ------------------------------------------------------------------------------'\n",
    "            print 'Model: %s \\n' %(self.model_file_name)\n",
    "            print('[Test %d/%d]: Accuracy Score = %2.3f, F1 Score = %2.3f \\n' %( \\\n",
    "                   test_i,len(dataset_lists)-1, current_accuracy_score, current_f1_score) )\n",
    "            print(confusion_matrix(my_features.y, predictions))\n",
    "            \n",
    "            print(classification_report(my_features.y, predictions))\n",
    "            print ' ------------------------------------------------------------------------------\\n'\n",
    "\n",
    "        average_accuracy_score = np.array(accuracy_test_scores).mean()\n",
    "        average_f1_score = np.array(f1_test_scores).mean()\n",
    "        print ' *****************************************************************************************'\n",
    "        print '  %s  ' %(self.model_file_name)\n",
    "        print '  scores: accuracy=%2.3f, f1=%2.3f' %(average_accuracy_score, average_f1_score)\n",
    "        print ' *****************************************************************************************'\n",
    "        # ---------------------------\n",
    "        # Generate Global results file -> keep track of previous results\n",
    "        # structure: \"precision result\" \"model which was used for results\"\n",
    "        current_result_line = '%2.4f %s\\n' %(average_f1_score, current_model_name)\n",
    "        \n",
    "        if os.path.isfile(self.global_results_file):\n",
    "            # Previous file exists\n",
    "            with open(self.global_results_file,'r') as f:\n",
    "                result_lines = f.readlines(); # contents of previous results\n",
    "                found_previous_results_for_this_model = False\n",
    "\n",
    "                for i in range(len(result_lines)):\n",
    "                    previous_result_line = result_lines[i].strip() # strip the end of line\n",
    "                    previous_model_name = previous_result_line.split(' ')[1]\n",
    "\n",
    "                    if current_model_name == previous_model_name:\n",
    "                        result_lines[i] = current_result_line\n",
    "                        found_previous_results_for_this_model =True\n",
    "                        break\n",
    "                if not found_previous_results_for_this_model:\n",
    "                    result_lines.append(current_result_line)\n",
    "                # Resort to display in nicer format\n",
    "                sorted_results = natsorted(result_lines, reverse=True)\n",
    "            with open(self.global_results_file,'w') as f:\n",
    "                # Rewrite file with new results\n",
    "                f.writelines(sorted_results)                \n",
    "        else:\n",
    "            with open(self.global_results_file,'w') as f:\n",
    "                f.writelines(current_result_line)\n",
    "        return current_f1_score\n",
    "                              \n",
    "class plant_detection:\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, work_dir ='/scratch/data/'):       \n",
    "        self.default_work_dir    = work_dir      \n",
    "        \n",
    "        # Define our Feature extraction methods\n",
    "        self.e_method    = [ \"rui\",\n",
    "                             \"rui-filtered\"]\n",
    "        self.e_params    = [ [(1,8),(2,16),(3,24)],\n",
    "                             [(1,8),(2,16),(3,24)]\n",
    "                           ]\n",
    "    \n",
    "        # Define our Classification methods\n",
    "        self.c_method = [ \"linear_SVM\",  \n",
    "                          \"poly_SVM\"  ,\n",
    "                          \"RBF_SVM\" ,   \n",
    "                        ]\n",
    "        self.c_params = [# Params for linear_SVM\n",
    "                          dict([('kernel','linear' ),\n",
    "                               ('C'     , 10      ),\n",
    "                               ('gamma' ,0.000001 )]),\n",
    "                          # Params for poly_SVM\n",
    "                          dict([('kernel','poly'),\n",
    "                               ('C'     , 10      ),\n",
    "                               ('gamma' ,0.000001 ),\n",
    "                               ('degree',2        )]),\n",
    "                          # Params for RBF_SVM\n",
    "                          dict([('kernel','rbf'    ),\n",
    "                               ('C'     , 30      ),\n",
    "                               ('gamma' ,0.00001 )]),\n",
    "                         ]\n",
    "        # Define our Image pre-processing methods\n",
    "        self.p_method    = [ \"pass\",\n",
    "                             \"contours\",\n",
    "                             \"cmask\",\n",
    "                             \"comb\"]\n",
    "        self.p_params    = [ dict(),\n",
    "                             dict([('thick',2)]),\n",
    "                             dict([('thick',2)]),\n",
    "                             dict([('thick',2),('k',0.2)])\n",
    "                           ]\n",
    "        # Create dictionaries (for easier indexing)\n",
    "        self.e_dict = dict( [ ( self.e_method[i],i ) for i in range(len(self.e_method)) ] )\n",
    "        self.c_dict = dict( [ ( self.c_method[i],i ) for i in range(len(self.c_method)) ] )     \n",
    "        self.p_dict = dict( [ ( self.p_method[i],i ) for i in range(len(self.p_method)) ] )\n",
    "        \n",
    "        self.initialized = False\n",
    "    \n",
    "    def initialize(self):\n",
    "        # Create Feature Extractors\n",
    "        self.e_list   = list()\n",
    "        for i in range (len(self.e_method)):\n",
    "            self.e = plant_feature_extraction(self.e_method[i],self.e_params[i])\n",
    "            self.e_list.extend ([self.e] )\n",
    "            \n",
    "        # Create Classifiers\n",
    "        self.c_list   = list()\n",
    "        for i in range (len(self.c_method)):\n",
    "            self.c = plant_classifier(self.c_method[i],self.c_params[i])\n",
    "            self.c_list.extend ([self.c] )\n",
    "            \n",
    "        # Create Preprocessing classes\n",
    "        self.p_list      = list()\n",
    "        for i in range (len(self.p_method)):\n",
    "            self.p = image_preprocess(self.p_method[i],self.p_params[i])\n",
    "            self.p_list.extend ([self.p] )\n",
    "        \n",
    "        self.initialized = True\n",
    "                                    \n",
    "    def extract_features(self, e_method='rui', p_method='pass', work_dir='default'):\n",
    "        if work_dir == 'default':\n",
    "            work_dir = self.default_work_dir\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        # Define our dataset training/testing names\n",
    "        self.e = self.e_list[self.e_dict[e_method]]\n",
    "        preprocess_class = self.p_list[self.p_dict[p_method]]\n",
    "        self.e.extract_features(preprocess_class, work_dir)\n",
    "        \n",
    "    def generate_model (self, e_method='rui', c_method='linear_SVM', p_method='pass', work_dir='default'):\n",
    "        print \" Generating model for e_method = '%s' , c_method ='%s' , work_dir = '%s' \" %(e_method,c_method, work_dir)\n",
    "        # Make sure that we have extracted the features\n",
    "        self.extract_features(e_method,p_method, work_dir)    \n",
    "        # Call the classifier with given feature extraction method\n",
    "        self.c = self.c_list[self.c_dict[c_method]]\n",
    "        self.c.train(self.e)\n",
    "        \n",
    "    def prediction (self, e_method ='rui', c_method='linear_SVM', p_method='pass', out_dir='default'):\n",
    "        # Make sure that we have extracted the features\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        self.e = self.e_list[self.e_dict[e_method]]\n",
    "        self.extract_features(e_method, p_method, self.e.work_dir)    \n",
    "        # call the classifier with given feature extraction method\n",
    "        self.c = self.c_list[self.c_dict[c_method]]\n",
    "        self.c.predict(self.e)\n",
    "        \n",
    "    def set_classifier_params(self,method, param_name, param_value):\n",
    "        classifier_params_index = self.c_dict[method]\n",
    "        (self.c_params[classifier_params_index])[param_name] = param_value\n",
    "        \n",
    "    def set_preprocess_params(self,method, param_name, param_value):\n",
    "        params_index = self.p_dict[method]\n",
    "        (self.p_params[params_index])[param_name] = param_value\n",
    "        \n",
    "    def print_global_results(self):\n",
    "        # Helper function to print updated global results file:\n",
    "        print '\\n ______________________________________________________________________________________\\n'\n",
    "        print '          Contents of Global results: '\n",
    "        with open(self.c.global_results_file,'r') as f:\n",
    "            my_lines = f.readlines()\n",
    "            for my_line in my_lines:\n",
    "                print my_line\n",
    "        \n",
    "\n",
    "# ----------------------------------------------------------------------------------------       \n",
    "# The above will go into a module when finalised.\n",
    "# The code below is used to input all parameters and generates the results of experiments.\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "work_dir = '/scratch/data/filtered_crossval_bccr/work/'\n",
    "#work_dir = \"/scratch/data/bccr_dataset_can_rad/work/\"\n",
    "\n",
    "my_extractor  = 'rui-filtered'\n",
    "my_classifiers = ['RBF_SVM']\n",
    "my_preprocess  = ['pass','cmask', 'comb']\n",
    "my_C_list = [10, 30, 60, 100, 1000]\n",
    "my_gamma_list= [1e-5, 1e-6]\n",
    "my_k = [0.1, 0.2, 0.5, 0.7, 0.8, 1, 2]\n",
    "\n",
    "for my_classifier in my_classifiers:\n",
    "    for my_preproc in my_preprocess:\n",
    "        for my_C in my_C_list:\n",
    "            for my_gamma in my_gamma_list:\n",
    "                if my_preproc == 'comb':\n",
    "                    given_k = my_k\n",
    "                else:\n",
    "                    given_k = [\"None\"]\n",
    "                for k in given_k:\n",
    "                    print \" ==============================================================================\"\n",
    "                    print \" Using CLS: '%s' , FE: '%s', PP: '%s' \" %(my_classifier, my_extractor, my_preproc)\n",
    "                    print \" gamma= %f, C=%d , k = %s\" %(my_gamma,my_C,str(k))\n",
    "\n",
    "                    a_pp = plant_detection(work_dir = work_dir)\n",
    "\n",
    "                    # override the default C and gamma from the classifer definition\n",
    "                    a_pp.set_classifier_params(my_classifier, 'gamma', my_gamma)\n",
    "                    a_pp.set_classifier_params(my_classifier, 'C', my_C)\n",
    "\n",
    "                    if k !=\"None\":\n",
    "                        # Override k param in preprocess class (if needed)\n",
    "                        a_pp.set_preprocess_params(my_preproc,'k',k)\n",
    "\n",
    "                    # Generate model and get score\n",
    "                    a_pp.generate_model(e_method=my_extractor, c_method=my_classifier, p_method=my_preproc)\n",
    "                    a_pp.prediction(e_method=my_extractor, c_method=my_classifier, p_method=my_preproc)\n",
    "\n",
    "a_pp.print_global_results()\n",
    "\n",
    "print '\\nALL DONE'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
